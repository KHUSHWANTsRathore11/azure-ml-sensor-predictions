# Component-Based Build Pipeline - Train Once, Deploy Everywhere
# 
# Branch Strategy:
#   develop    ‚Üí Train in Dev workspace + Promote to Registry (with approval)
#   release/*  ‚Üí Deploy from Registry to Test endpoints (with approvals)
#   main       ‚Üí Deploy from Registry to Prod endpoints (with approval)

trigger:
  branches:
    include:
      - develop
      - release/*
      - main
  paths:
    include:
      - config/circuits.yaml
      - config/environment.yaml
      - components/**
      - src/packages/**

pr:
  branches:
    include:
      - develop
      - release/*
      - main

parameters:
  - name: manualCircuits
    displayName: 'Manual Circuit Specification (Optional)'
    type: string
    default: ''
# Format: plant1_circuit1,plant2_circuit2
# Example: flottec_2110,flottec_2130
# Leave empty to use automatic change detection

variables:
  - name: pythonVersion
    value: '3.9'
  
  # Long-running job notification interval (in hours)
  - name: longRunningJobNotificationIntervalHours
    value: '4'
  
  # Optional: Email notification recipients (comma-separated)
  # Set this in pipeline variables or variable group
  - name: mlEngineersEmail
    value: ''  # e.g., 'ml-team@company.com,ops-team@company.com'
  
  # Branch-to-environment mapping
  - name: targetEnvironment
    ${{ if eq(variables['Build.SourceBranch'], 'refs/heads/develop') }}:
      value: 'dev'
    ${{ elseif startsWith(variables['Build.SourceBranch'], 'refs/heads/release/') }}:
      value: 'test'
    ${{ elseif eq(variables['Build.SourceBranch'], 'refs/heads/main') }}:
      value: 'prod'
    ${{ else }}:
      value: 'dev'
  
  # Load environment-specific and registry variable groups
  - ${{ if eq(variables['Build.SourceBranch'], 'refs/heads/develop') }}:
      - group: mlops-dev-variables
      - group: mlops-registry-variables
  - ${{ if startsWith(variables['Build.SourceBranch'], 'refs/heads/release/') }}:
      - group: mlops-test-variables
      - group: mlops-registry-variables
  - ${{ if eq(variables['Build.SourceBranch'], 'refs/heads/main') }}:
      - group: mlops-prod-variables
      - group: mlops-registry-variables

stages:
  # ============================================
  # PR VALIDATION: Run on Pull Request to develop branch
  # (release/* and main PRs don't need validation - they only deploy from Registry)
  # ============================================
    - stage: PRValidation
      displayName: 'PR Validation (develop only)'
      condition: and(eq(variables['Build.Reason'], 'PullRequest'), eq(variables['System.PullRequest.TargetBranch'], 'refs/heads/develop'))
      jobs:
        - job: ValidateConfigs
          displayName: 'Validate Configuration Files'
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - checkout: self
              fetchDepth: 0
          
            - task: UsePythonVersion@0
              inputs:
                versionSpec: '$(pythonVersion)'
          
            - script: pip install pyyaml jsonschema
              displayName: 'Install dependencies'
          
          # Validate circuits.yaml structure
            - script: |
                echo "üìã Validating circuits.yaml..."
                python3 << 'EOF'
                import yaml
                import sys
              
                try:
                    with open('config/circuits.yaml', 'r') as f:
                        config = yaml.safe_load(f)
                  
                    circuits = config.get('circuits', [])
                  
                    if not circuits:
                        print("‚ùå No circuits found in config")
                        sys.exit(1)
                  
                    required_fields = ['plant_id', 'circuit_id', 'cutoff_date', 'model_name']
                  
                    for i, circuit in enumerate(circuits):
                        for field in required_fields:
                            if field not in circuit:
                                print(f"‚ùå Circuit {i+1} missing required field: {field}")
                                sys.exit(1)
                      
                      # Validate cutoff_date format (YYYY-MM-DD)
                        cutoff = circuit.get('cutoff_date', '')
                        if len(cutoff) != 10 or cutoff[4] != '-' or cutoff[7] != '-':
                            print(f"‚ùå Circuit {circuit['plant_id']}/{circuit['circuit_id']}: Invalid cutoff_date format. Expected YYYY-MM-DD")
                            sys.exit(1)
                  
                    print(f"‚úÖ Validated {len(circuits)} circuits")
                  
                except yaml.YAMLError as e:
                    print(f"‚ùå YAML syntax error: {e}")
                    sys.exit(1)
                except Exception as e:
                    print(f"‚ùå Validation error: {e}")
                    sys.exit(1)
                EOF
              displayName: 'Validate circuits.yaml'
          
          # Validate environment.yaml
            - script: |
                echo "üê≥ Validating environment.yaml..."
                python3 << 'EOF'
                import yaml
                import sys
              
                try:
                    with open('config/environment.yaml', 'r') as f:
                        env_config = yaml.safe_load(f)
                  
                  # Check required fields
                    if 'name' not in env_config:
                        print("‚ùå Missing 'name' field in environment.yaml")
                        sys.exit(1)
                  
                    if 'version' not in env_config:
                        print("‚ùå Missing 'version' field in environment.yaml")
                        sys.exit(1)
                  
                    print(f"‚úÖ Environment config valid: {env_config['name']}:{env_config['version']}")
                  
                except yaml.YAMLError as e:
                    print(f"‚ùå YAML syntax error: {e}")
                    sys.exit(1)
                except Exception as e:
                    print(f"‚ùå Validation error: {e}")
                    sys.exit(1)
                EOF
              displayName: 'Validate environment.yaml'
          
          # Validate component YAML files
            - script: |
                echo "üì¶ Validating component files..."
              
                FAILED=0
                for comp_file in components/*/component.yaml; do
                  if [ -f "$comp_file" ]; then
                    echo "Checking: $comp_file"
                  
                    python3 << EOF
                import yaml
                import sys
              
                try:
                    with open('$comp_file', 'r') as f:
                        comp = yaml.safe_load(f)
                  
                    required = ['name', 'version', 'type']
                    for field in required:
                        if field not in comp:
                            print(f"‚ùå Missing field: {field}")
                            sys.exit(1)
                  
                    print(f"‚úÖ Valid: {comp['name']}:{comp['version']}")
              
                except Exception as e:
                    print(f"‚ùå Error: {e}")
                    sys.exit(1)
                EOF
                  
                    if [ $? -ne 0 ]; then
                      FAILED=1
                    fi
                  fi
                done
              
              # Validate pipeline component
                if [ -f "components/training-pipeline-component.yaml" ]; then
                  echo "Checking: components/training-pipeline-component.yaml"
                  python3 << 'EOF'
                import yaml
                import sys
              
                try:
                    with open('components/training-pipeline-component.yaml', 'r') as f:
                        comp = yaml.safe_load(f)
                  
                    if comp.get('type') != 'pipeline':
                        print("‚ùå Pipeline component must have type: pipeline")
                        sys.exit(1)
                  
                    print(f"‚úÖ Valid: {comp['name']}:{comp['version']}")
              
                except Exception as e:
                    print(f"‚ùå Error: {e}")
                    sys.exit(1)
                EOF
                
                  if [ $? -ne 0 ]; then
                    FAILED=1
                  fi
                fi
              
                if [ $FAILED -eq 1 ]; then
                  echo "‚ùå Component validation failed"
                  exit 1
                fi
              
                echo "‚úÖ All component files valid"
              displayName: 'Validate component files'
          
          # Validate environment-specific configs
            - script: |
                echo "‚öôÔ∏è  Validating environment configurations..."
              
                FAILED=0
                for env_file in config/environments/*.yaml; do
                  if [ -f "$env_file" ]; then
                    echo "Checking: $env_file"
                  
                    python3 << EOF
                import yaml
                import sys
              
                try:
                    with open('$env_file', 'r') as f:
                        env_config = yaml.safe_load(f)
                  
                  # Check Azure section
                    if 'azure' not in env_config:
                        print("‚ùå Missing 'azure' section")
                        sys.exit(1)
                  
                    azure = env_config['azure']
                  
                  # Validate compute clusters
                    if 'compute' not in azure:
                        print("‚ùå Missing 'compute' configuration")
                        sys.exit(1)
                  
                    compute = azure['compute']
                    required_compute = ['training_cluster', 'scoring_cluster', 
                                       'training_vm_size', 'scoring_vm_size',
                                       'training_min_nodes', 'training_max_nodes']
                  
                    for field in required_compute:
                        if field not in compute:
                            print(f"‚ùå Missing compute field: {field}")
                            sys.exit(1)
                  
                  # Validate node counts
                    if compute['training_min_nodes'] < 0:
                        print("‚ùå training_min_nodes must be >= 0")
                        sys.exit(1)
                  
                    if compute['training_max_nodes'] < compute['training_min_nodes']:
                        print("‚ùå training_max_nodes must be >= training_min_nodes")
                        sys.exit(1)
                  
                  # Validate VM sizes (common Azure ML sizes)
                    valid_vm_sizes = ['Standard_DS2_v2', 'Standard_DS3_v2', 'Standard_DS4_v2', 
                                     'Standard_DS12_v2', 'Standard_DS13_v2', 'Standard_DS14_v2',
                                     'Standard_NC6', 'Standard_NC12', 'Standard_NC24']
                  
                    if compute['training_vm_size'] not in valid_vm_sizes:
                        print(f"‚ö†Ô∏è  Warning: Unusual VM size for training: {compute['training_vm_size']}")
                  
                    print(f"‚úÖ Valid: {env_config['environment']['name']} environment")
                    print(f"   Training: {compute['training_cluster']} ({compute['training_vm_size']})")
                    print(f"   Scoring: {compute['scoring_cluster']} ({compute['scoring_vm_size']})")
              
                except Exception as e:
                    print(f"‚ùå Error: {e}")
                    sys.exit(1)
                EOF
                  
                    if [ $? -ne 0 ]; then
                      FAILED=1
                    fi
                  fi
                done
              
                if [ $FAILED -eq 1 ]; then
                  echo "‚ùå Environment configuration validation failed"
                  exit 1
                fi
              
                echo "‚úÖ All environment configurations valid"
              displayName: 'Validate environment compute configs'
          
          # Validate pipeline compute references
            - script: |
                echo "üîß Validating pipeline compute references..."
              
                python3 << 'EOF'
                import yaml
                import sys
                import os
              
                try:
                  # Check pipeline files for compute references
                    pipeline_files = []
                    if os.path.exists('pipelines'):
                        for file in os.listdir('pipelines'):
                            if file.endswith('.yaml') or file.endswith('.yml'):
                                pipeline_files.append(os.path.join('pipelines', file))
                  
                    for pipeline_file in pipeline_files:
                        with open(pipeline_file, 'r') as f:
                            pipeline = yaml.safe_load(f)
                      
                      # Check if compute is specified
                        if 'settings' in pipeline and 'default_compute' in pipeline['settings']:
                            compute = pipeline['settings']['default_compute']
                          
                          # Check if it's a valid format (azureml:cluster-name)
                            if compute.startswith('azureml:'):
                                cluster_name = compute.replace('azureml:', '')
                                print(f"‚úÖ {os.path.basename(pipeline_file)}: References compute '{cluster_name}'")
                            else:
                                print(f"‚ö†Ô∏è  {os.path.basename(pipeline_file)}: Compute reference should start with 'azureml:'")
                        else:
                            print(f"‚ö†Ô∏è  {os.path.basename(pipeline_file)}: No default_compute specified")
                  
                    if not pipeline_files:
                        print("‚ÑπÔ∏è  No pipeline files found in pipelines/")
                    else:
                        print(f"\n‚úÖ Validated {len(pipeline_files)} pipeline file(s)")
              
                except Exception as e:
                    print(f"‚ùå Error: {e}")
                    sys.exit(1)
                EOF
              displayName: 'Validate pipeline compute references'
          
          # Check ML asset versions against Dev workspace
            - task: AzureCLI@2
              displayName: 'Check ML Asset Versions'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                inlineScript: |
                  echo "üîç Checking if ML asset versions already exist in Dev workspace..."
                
                # Determine which workspace to check based on target branch
                  TARGET_BRANCH=$(System.PullRequest.TargetBranch)
                
                  if [[ "$TARGET_BRANCH" == "develop" ]]; then
                    WORKSPACE=$(mlopsDevWorkspace)
                    RG=$(mlopsDevResourceGroup)
                    ENV_NAME="Dev"
                  elif [[ "$TARGET_BRANCH" == release/* ]]; then
                    WORKSPACE=$(mlopsTestWorkspace)
                    RG=$(mlopsTestResourceGroup)
                    ENV_NAME="Test"
                  elif [[ "$TARGET_BRANCH" == "main" ]]; then
                    WORKSPACE=$(mlopsProdWorkspace)
                    RG=$(mlopsProdResourceGroup)
                    ENV_NAME="Prod"
                  else
                    echo "‚ö†Ô∏è  Unknown target branch: $TARGET_BRANCH"
                    echo "   Skipping version checks"
                    exit 0
                  fi
                
                  echo "Target workspace: $WORKSPACE ($ENV_NAME)"
                  echo ""
                
                  WARNINGS=0
                
                # Check Environment version
                  echo "üì¶ Checking Environment version..."
                  ENV_VERSION=$(grep "^version:" config/environment.yaml | awk '{print $2}' | tr -d '"' | tr -d "'")
                  ENV_NAME_VAL=$(grep "^name:" config/environment.yaml | awk '{print $2}' | tr -d '"' | tr -d "'")
                
                  if az ml environment show \
                    --name "$ENV_NAME_VAL" \
                    --version "$ENV_VERSION" \
                    --workspace-name "$WORKSPACE" \
                    --resource-group "$RG" &>/dev/null; then
                    echo "‚ö†Ô∏è  WARNING: Environment $ENV_NAME_VAL:$ENV_VERSION already exists in $ENV_NAME workspace"
                    echo "   Consider incrementing the version if you made changes"
                    WARNINGS=$((WARNINGS + 1))
                  else
                    echo "‚úÖ Environment $ENV_NAME_VAL:$ENV_VERSION is a new version"
                  fi
                
                  echo ""
                
                # Check Pipeline Component version
                  echo "üì¶ Checking Pipeline Component version..."
                  COMP_NAME=$(grep "^name:" components/training-pipeline-component.yaml | awk '{print $2}' | tr -d '"' | tr -d "'")
                  COMP_VERSION=$(grep "^version:" components/training-pipeline-component.yaml | awk '{print $2}' | tr -d '"' | tr -d "'")
                
                  if az ml component show \
                    --name "$COMP_NAME" \
                    --version "$COMP_VERSION" \
                    --workspace-name "$WORKSPACE" \
                    --resource-group "$RG" &>/dev/null; then
                    echo "‚ö†Ô∏è  WARNING: Pipeline component $COMP_NAME:$COMP_VERSION already exists in $ENV_NAME workspace"
                    echo "   Consider incrementing the version if you made changes"
                    WARNINGS=$((WARNINGS + 1))
                  else
                    echo "‚úÖ Pipeline component $COMP_NAME:$COMP_VERSION is a new version"
                  fi
                
                  echo ""
                
                # Check MLTable versions for changed circuits
                  if [ -f "changed_circuits.json" ]; then
                    echo "üì¶ Checking MLTable versions for changed circuits..."
                  
                    python3 << 'EOF'
                  import json
                  import subprocess
                  import sys
                
                  with open('changed_circuits.json', 'r') as f:
                      changed = json.load(f)
                
                  circuits = changed.get('circuits', [])
                  warnings = 0
                
                  for circuit in circuits:
                      plant_id = circuit['plant_id']
                      circuit_id = circuit['circuit_id']
                      cutoff_date = circuit.get('cutoff_date', '')
                    
                      if not cutoff_date:
                          continue
                    
                      data_name = f"{plant_id}_{circuit_id}"
                      data_version = cutoff_date
                    
                      check_cmd = [
                          'az', 'ml', 'data', 'show',
                          '--name', data_name,
                          '--version', data_version,
                          '--workspace-name', '$WORKSPACE',
                          '--resource-group', '$RG'
                      ]
                    
                      result = subprocess.run(check_cmd, capture_output=True, text=True)
                    
                      if result.returncode == 0:
                          print(f"‚ö†Ô∏è  WARNING: MLTable {data_name}:{data_version} already exists")
                          print(f"   This will skip re-registration during pipeline run")
                          warnings += 1
                      else:
                          print(f"‚úÖ MLTable {data_name}:{data_version} is a new version")
                
                  sys.exit(warnings)
                  EOF
                  
                    MLTABLE_WARNINGS=$?
                    WARNINGS=$((WARNINGS + MLTABLE_WARNINGS))
                    echo ""
                  fi
                
                # Summary
                  if [ $WARNINGS -gt 0 ]; then
                    echo "‚ö†Ô∏è  Found $WARNINGS version conflict(s)"
                    echo "   Review the warnings above. If versions are intentionally reused, you can proceed."
                    echo "   If you made changes, consider incrementing version numbers."
                  else
                    echo "‚úÖ All ML asset versions are new - no conflicts detected"
                  fi
                
                  echo ""
                  echo "‚ÑπÔ∏è  Note: This is a warning check only. Pipeline will not fail."
              continueOnError: true
          
          # Lint Python code
            - script: |
                echo "üîç Checking Python code quality..."
                pip install flake8
              
              # Run flake8 with relaxed rules for initial validation
                flake8 scripts/ src/ --count --select=E9,F63,F7,F82 --show-source --statistics || true
              
                echo "‚úÖ Python linting complete"
              displayName: 'Lint Python code'
          
          # Check for changes and validate
            - script: |
                echo "üîç Detecting changes for PR validation..."
                python3 scripts/detect_config_changes.py --target-branch $(System.PullRequest.TargetBranch) || echo "‚ö†Ô∏è  Could not detect changes (may be first commit)"
              
                if [ -f "changed_circuits.json" ]; then
                  echo "üìä Changes detected:"
                  cat changed_circuits.json
                fi
              displayName: 'Detect configuration changes'
              continueOnError: true

  # ============================================
  # DEVELOP BRANCH: Train in Dev + Promote to Registry
  # ============================================
  - ${{ if eq(variables['Build.SourceBranch'], 'refs/heads/develop') }}:
  
      - stage: RegisterInfrastructure
        displayName: '[DEV] Register Infrastructure'
        condition: ne(variables['Build.Reason'], 'PullRequest')  # Skip on PRs
        jobs:
        - job: RegisterEnvAndComponents
          displayName: 'Register Environment and Components'
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - checkout: self
              fetchDepth: 0
          
            - task: UsePythonVersion@0
              inputs:
                versionSpec: '$(pythonVersion)'
          
            - script: pip install pyyaml
              displayName: 'Install dependencies'
          
            - task: AzureCLI@2
              displayName: 'Register Custom Environment'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                inlineScript: |
                  echo "üê≥ Checking environment in Dev workspace..."
                
                # Get version from config file
                  CONFIG_VERSION=$(grep "^version:" config/environment.yaml | awk '{print $2}' | tr -d '"' | tr -d "'")
                  ENV_NAME=$(grep "^name:" config/environment.yaml | awk '{print $2}' | tr -d '"' | tr -d "'")
                  echo "Environment: $ENV_NAME:$CONFIG_VERSION"
                
                # Check if this version already exists
                  EXISTING_ENV=$(az ml environment show \
                    --name "$ENV_NAME" \
                    --version "$CONFIG_VERSION" \
                    --workspace-name $(workspaceName) \
                    --resource-group $(resourceGroup) 2>/dev/null || echo "not_found")
                
                  NEW_ENV_CREATED="false"
                
                  if [[ "$EXISTING_ENV" == "not_found" ]]; then
                    echo "Version $CONFIG_VERSION not found. Registering new environment..."
                    az ml environment create \
                      --file config/environment.yaml \
                      --workspace-name $(workspaceName) \
                      --resource-group $(resourceGroup)
                    echo "‚úÖ Environment registered: $ENV_NAME:$CONFIG_VERSION"
                    NEW_ENV_CREATED="true"
                  else
                    echo "‚úÖ Environment $ENV_NAME:$CONFIG_VERSION already exists. Skipping registration."
                  fi
                
                # Save environment info for promotion stage
                # Convert bash boolean to JSON boolean
                  if [ "$NEW_ENV_CREATED" = "true" ]; then
                    JSON_BOOL="true"
                  else
                    JSON_BOOL="false"
                  fi
                
                  cat > environment_info.json << ENVEOF
                  {
                    "name": "$ENV_NAME",
                    "version": "$CONFIG_VERSION",
                    "newly_created": $JSON_BOOL
                  }
                  ENVEOF
                
                  echo ""
                  echo "üìù Environment info saved:"
                  cat environment_info.json
                  echo ""
                
                  echo "##vso[task.setvariable variable=envVersion;isOutput=true]$CONFIG_VERSION"
                  echo "##vso[task.setvariable variable=envNewlyCreated;isOutput=true]$NEW_ENV_CREATED"
              name: registerEnv
          
            - task: AzureCLI@2
              displayName: 'Register Pipeline Component'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                inlineScript: |
                  echo "üì¶ Checking and registering pipeline component in Dev workspace..."
                
                  PIPELINE_COMP_FILE="components/training-pipeline-component.yaml"
                
                # Extract name and version from pipeline component file
                  COMP_NAME=$(grep "^name:" "$PIPELINE_COMP_FILE" | awk '{print $2}' | tr -d '"' | tr -d "'")
                  COMP_VERSION=$(grep "^version:" "$PIPELINE_COMP_FILE" | awk '{print $2}' | tr -d '"' | tr -d "'")
                
                  echo "Checking pipeline component: $COMP_NAME:$COMP_VERSION"
                
                # Check if this version already exists
                  EXISTING_COMP=$(az ml component show \
                    --name "$COMP_NAME" \
                    --version "$COMP_VERSION" \
                    --workspace-name $(workspaceName) \
                    --resource-group $(resourceGroup) 2>/dev/null || echo "not_found")
                
                  if [[ "$EXISTING_COMP" == "not_found" ]]; then
                    echo "Version $COMP_VERSION not found. Registering..."
                  
                    if az ml component create \
                      --file "$PIPELINE_COMP_FILE" \
                      --workspace-name $(workspaceName) \
                      --resource-group $(resourceGroup); then
                      echo "‚úÖ Registered pipeline component: $COMP_NAME:$COMP_VERSION"
                    else
                      echo "‚ùå Failed to register pipeline component: $COMP_NAME:$COMP_VERSION"
                      exit 1
                    fi
                  else
                    echo "‚úÖ Pipeline component $COMP_NAME:$COMP_VERSION already exists. Skipping."
                  fi
                
                  echo "##vso[task.setvariable variable=pipelineCompVersion;isOutput=true]$COMP_VERSION"
              name: registerPipelineComp
          
            - script: python3 scripts/generate_circuit_configs.py
              displayName: 'Generate Circuit Configs'
          
            - task: AzureCLI@2
              displayName: 'Detect Changed Circuits & Register MLTables'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                inlineScript: |
                  echo "üîç Detecting changed circuits..."
                
                # Check for manual circuit specification
                  MANUAL_CIRCUITS="${{ parameters.manualCircuits }}"
                
                  if [ -n "$MANUAL_CIRCUITS" ]; then
                    echo "üìù Manual circuit specification provided: $MANUAL_CIRCUITS"
                    echo "   Bypassing automatic change detection"
                  
                  # Create manual changed_circuits.json
                    python3 << 'MANUAL_EOF'
                    import json
                    import yaml
                    import sys

                    manual_circuits = "$MANUAL_CIRCUITS"
                    circuit_list = [c.strip() for c in manual_circuits.split(',') if c.strip()]

                    if not circuit_list:
                        print("‚ùå Manual circuits specified but list is empty")
                        sys.exit(1)

                    print(f"Processing {len(circuit_list)} manually specified circuit(s)...\n")

                  # Load master circuits config
                    with open('config/circuits.yaml', 'r') as f:
                        master_config = yaml.safe_load(f)

                    circuits_data = []
                    for circuit_key in circuit_list:
                      # Find circuit in master config
                        found = False
                        for circuit in master_config.get('circuits', []):
                            plant_id = circuit.get('plant_id', '')
                            circuit_id = circuit.get('circuit_id', '')
                            key = f"{plant_id}_{circuit_id}"
                          
                            if key == circuit_key:
                                circuits_data.append({
                                    'plant_id': plant_id,
                                    'circuit_id': circuit_id,
                                    'cutoff_date': circuit.get('cutoff_date', ''),
                                    'model_name': circuit.get('model_name', f'{plant_id.lower()}-{circuit_id.lower()}')
                                })
                                print(f"‚úÖ Found: {key}")
                                found = True
                                break
                      
                        if not found:
                            print(f"‚ùå Circuit not found in config/circuits.yaml: {circuit_key}")
                            sys.exit(1)

                  # Create changed_circuits.json
                    output = {'circuits': circuits_data}
                    with open('changed_circuits.json', 'w') as f:
                        json.dump(output, f, indent=2)

                    print(f"\n‚úÖ Created changed_circuits.json with {len(circuits_data)} circuit(s)")
                    MANUAL_EOF
                  else
                    echo "üîç Using automatic change detection"
                    python3 scripts/detect_config_changes.py
                  fi
                
                # Register MLTable for each changed circuit
                  if [ -f "changed_circuits.json" ]; then
                    echo "üìä Registering MLTables for changed circuits..."
                    python3 << 'EOF'
                  import json
                  import subprocess
                  import sys
                
                  with open('changed_circuits.json', 'r') as f:
                      changed = json.load(f)
                
                  circuits = changed.get('circuits', [])
                
                  if not circuits:
                      print("‚ÑπÔ∏è  No circuits to register")
                      sys.exit(0)
                
                  failed = False
                
                  for circuit in circuits:
                      plant_id = circuit['plant_id']
                      circuit_id = circuit['circuit_id']
                      cutoff_date = circuit.get('cutoff_date', '')
                    
                      if not cutoff_date:
                          print(f"‚ö†Ô∏è  Skipping {plant_id}_{circuit_id}: no cutoff_date")
                          continue
                    
                      data_name = f"{plant_id}_{circuit_id}"
                      data_version = cutoff_date
                    
                      print(f"\nChecking MLTable: {data_name}:{data_version}")
                    
                    # Check if this version already exists
                      check_cmd = [
                          'az', 'ml', 'data', 'show',
                          '--name', data_name,
                          '--version', data_version,
                          '--workspace-name', '$(workspaceName)',
                          '--resource-group', '$(resourceGroup)'
                      ]
                    
                      check_result = subprocess.run(check_cmd, capture_output=True, text=True)
                    
                      if check_result.returncode == 0:
                          print(f"‚úÖ MLTable {data_name}:{data_version} already exists. Skipping.")
                          continue
                    
                    # Version doesn't exist, register it
                      print(f"Registering MLTable: {data_name}:{data_version}")
                    
                      create_cmd = [
                          'az', 'ml', 'data', 'create',
                          '--name', data_name,
                          '--version', data_version,
                          '--type', 'mltable',
                          '--path', f'azureml://datastores/workspaceblobstore/paths/mltable/{plant_id}_{circuit_id}/',
                          '--workspace-name', '$(workspaceName)',
                          '--resource-group', '$(resourceGroup)'
                      ]
                    
                      result = subprocess.run(create_cmd, capture_output=True, text=True)
                      if result.returncode != 0:
                          print(f'‚ùå Failed to register {data_name}:{data_version}')
                          print(f'   Error: {result.stderr}')
                          failed = True
                      else:
                          print(f'‚úÖ Registered: {data_name}:{data_version}')
                
                  if failed:
                      print("\n‚ùå Some MLTable registrations failed")
                      sys.exit(1)
                  else:
                      print("\n‚úÖ MLTable registration complete")
                  EOF
                  fi
              name: detectChanges
          
            - task: PublishPipelineArtifact@1
              inputs:
                targetPath: 'changed_circuits.json'
                artifact: 'changed_circuits'
          
            - task: PublishPipelineArtifact@1
              inputs:
                targetPath: 'environment_info.json'
                artifact: 'environment_info'
  
    - stage: TrainModels
      displayName: '[DEV] Train Models'
      dependsOn: RegisterInfrastructure
      condition: ne(variables['Build.Reason'], 'PullRequest')  # Skip on PRs
      jobs:
        - job: ParallelTraining
          displayName: 'Train Models in Parallel'
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - checkout: self
          
            - task: DownloadPipelineArtifact@2
              inputs:
                artifact: 'changed_circuits'
                path: $(Pipeline.Workspace)
          
            - task: AzureCLI@2
              displayName: 'Submit Training Jobs'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                inlineScript: |
                  echo "üöÄ Submitting training jobs for changed circuits..."
                
                  python3 << 'EOF'
                  import json
                  import subprocess
                  import time
                  import sys
                  import os
                
                  with open('$(Pipeline.Workspace)/changed_circuits.json', 'r') as f:
                      changed = json.load(f)
                
                  circuits = changed.get('circuits', [])
                
                  if not circuits:
                      print("‚ÑπÔ∏è  No circuits to train")
                      sys.exit(0)
                
                  submitted_jobs = []  # List of dicts with circuit info and job name
                  failed_submissions = []
                
                # Submit all jobs (continue on failure)
                  for circuit in circuits:
                      plant_id = circuit['plant_id']
                      circuit_id = circuit['circuit_id']
                      cutoff_date = circuit.get('cutoff_date', '')
                    
                      print(f"\nüìä Submitting training job: {plant_id}_{circuit_id}")
                    
                    # Circuit config path (generated in Task 1.3)
                      circuit_config = f'config/circuits/{plant_id}_{circuit_id}.yaml'
                    
                    # MLTable data asset reference (registered in Task 1.4)
                      data_name = f"{plant_id}_{circuit_id}"
                      data_version = cutoff_date
                      mltable_uri = f"azureml:{data_name}:{data_version}"
                    
                    # Job name with timestamp
                      job_name = f"{plant_id}_{circuit_id}_{cutoff_date}".replace('-', '_').replace(':', '_')
                    
                      print(f"   Config: {circuit_config}")
                      print(f"   Data: {mltable_uri}")
                    
                    # Submit training job using the pipeline job file
                      cmd = [
                          'az', 'ml', 'job', 'create',
                          '--file', 'pipelines/single-circuit-training.yaml',
                          '--name', job_name,
                          '--set', f'inputs.circuit_config.path={circuit_config}',
                          '--set', f'inputs.training_data.path={mltable_uri}',
                          '--workspace-name', '$(workspaceName)',
                          '--resource-group', '$(resourceGroup)',
                          '--query', 'name',
                          '-o', 'tsv'
                      ]
                    
                    # Retry logic: Try up to 3 times (1 initial + 2 retries)
                      max_retries = 2
                      retry_count = 0
                      success = False
                      last_error = None
                    
                      while retry_count <= max_retries and not success:
                          if retry_count > 0:
                              wait_time = 30 * retry_count  # 30s, 60s
                              print(f"   ‚è≥ Retry {retry_count}/{max_retries} after {wait_time}s wait...")
                              time.sleep(wait_time)
                        
                          result = subprocess.run(cmd, capture_output=True, text=True)
                        
                          if result.returncode == 0:
                              job_name = result.stdout.strip()
                              submitted_jobs.append({
                                  'job_name': job_name,
                                  'plant_id': plant_id,
                                  'circuit_id': circuit_id,
                                  'cutoff_date': cutoff_date,
                                  'model_name': circuit.get('model_name', f'{plant_id.lower()}-{circuit_id.lower()}')
                              })
                              print(f"   ‚úÖ Submitted: {job_name}")
                              success = True
                          else:
                              last_error = result.stderr
                              retry_count += 1
                              if retry_count <= max_retries:
                                  print(f"   ‚ö†Ô∏è  Attempt {retry_count} failed: {result.stderr[:100]}")
                    
                    # If all retries failed, cancel all submitted jobs and exit
                      if not success:
                          print(f"   ‚ùå Failed after {max_retries + 1} attempts: {last_error}")
                          print(f"\n{'='*60}")
                          print(f"‚ùå CRITICAL: Job submission failed for {plant_id}/{circuit_id}")
                          print(f"{'='*60}\n")
                        
                        # Cancel all previously submitted jobs
                          if submitted_jobs:
                              print(f"üõë Cancelling {len(submitted_jobs)} previously submitted job(s)...")
                              for job_info in submitted_jobs:
                                  cancel_cmd = [
                                      'az', 'ml', 'job', 'cancel',
                                      '--name', job_info['job_name'],
                                      '--workspace-name', '$(workspaceName)',
                                      '--resource-group', '$(resourceGroup)'
                                  ]
                                  cancel_result = subprocess.run(cancel_cmd, capture_output=True, text=True)
                                  if cancel_result.returncode == 0:
                                      print(f"   ‚úÖ Cancelled: {job_info['job_name']}")
                                  else:
                                      print(f"   ‚ö†Ô∏è  Could not cancel {job_info['job_name']}: {cancel_result.stderr[:100]}")
                              print()
                        
                          print(f"‚ùå Pipeline terminated due to job submission failure")
                          print(f"   Failed circuit: {plant_id}/{circuit_id}")
                          print(f"   Error: {last_error}")
                          sys.exit(1)
                
                # Save successfully submitted jobs for monitoring
                  with open('training_jobs.json', 'w') as f:
                      json.dump({'submitted_jobs': submitted_jobs}, f, indent=2)
                
                # Summary
                  print(f"\n{'='*60}")
                  print(f"Job Submission Summary:")
                  print(f"  ‚úÖ Successfully submitted: {len(submitted_jobs)} job(s)")
                  print(f"{'='*60}\n")
                
                  print("üìù Job details saved to training_jobs.json")
                  EOF
          
            - task: PublishPipelineArtifact@1
              displayName: 'Publish Job Names'
              inputs:
                targetPath: 'training_jobs.json'
                artifact: 'training_jobs'
      
      # Wait for training jobs using Python SDK
      # Uses AzureML Job Wait extension: https://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-v2
      # Each AzureML Job Wait task can only wait for ONE Azure ML job
      
      # IMPORTANT: Dynamic matrix with server jobs has limitations in Azure DevOps
      # Alternative: Use agent job with Python SDK to poll all jobs efficiently
        - job: WaitForTrainingJobs
          displayName: 'Wait for Training Jobs'
          dependsOn: ParallelTraining
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - task: DownloadPipelineArtifact@2
              inputs:
                artifact: 'training_jobs'
                path: $(Pipeline.Workspace)
          
            - task: AzureCLI@2
              name: monitorJobs
              displayName: 'Wait for All Training Jobs'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                addSpnToEnvironment: true
                inlineScript: |
                  echo "‚è≥ Waiting for training jobs to complete..."
                
                # Install Azure ML SDK
                  pip install -q azure-ai-ml azure-identity
                
                  python3 << 'EOF'
                  import json
                  import time
                  import sys
                  from azure.identity import DefaultAzureCredential
                  from azure.ai.ml import MLClient
                
                # Load submitted jobs
                  with open('$(Pipeline.Workspace)/training_jobs.json', 'r') as f:
                      data = json.load(f)
                
                  submitted_jobs = data.get('submitted_jobs', [])
                  failed_submissions = data.get('failed_submissions', [])
                
                  if failed_submissions:
                      print(f"‚ö†Ô∏è  Note: {len(failed_submissions)} job(s) failed to submit and will be skipped\n")
                
                  if not submitted_jobs:
                      print("‚ÑπÔ∏è  No jobs to wait for")
                      sys.exit(0)
                
                  job_names = [job['job_name'] for job in submitted_jobs]
                
                  print(f"üìä Waiting for {len(job_names)} training job(s)...\n")
                
                # Initialize ML Client
                  credential = DefaultAzureCredential()
                  ml_client = MLClient(
                      credential=credential,
                      subscription_id='$(subscriptionId)',
                      resource_group_name='$(resourceGroup)',
                      workspace_name='$(workspaceName)'
                  )
                
                # Track job status
                  pending = set(job_names)
                  completed = []
                  failed = []
                
                # Track job start time and last notification
                  from datetime import datetime, timedelta
                  import os
                
                  job_start_time = {}
                  last_notification_time = {}
                
                # Get notification interval from pipeline variable (default 4 hours)
                  notification_interval_hours = float(os.getenv('LONGRUNNINGJOBNOTIFICATIONINTERVALHOURS', '4'))
                  NOTIFICATION_INTERVAL = int(notification_interval_hours * 3600)  # Convert to seconds
                
                  print(f"üìä Notification interval: {notification_interval_hours} hours")
                  print()
                
                  for job_name in job_names:
                      job_start_time[job_name] = datetime.now()
                      last_notification_time[job_name] = datetime.now()
                
                # Poll jobs
                  poll_count = 0
                  while pending:
                      time.sleep(60)  # Check every minute
                      poll_count += 1
                    
                      current_time = datetime.now()
                    
                    # Status check every minute
                      print(f"‚è∞ Status check #{poll_count} - {len(pending)} job(s) remaining...")
                    
                      for job_name in list(pending):
                          try:
                              job = ml_client.jobs.get(job_name)
                              status = job.status
                            
                            # Calculate elapsed time
                              elapsed = (current_time - job_start_time[job_name]).total_seconds()
                              elapsed_hours = elapsed / 3600
                            
                            # Check if job is long-running and needs notification
                              time_since_notification = (current_time - last_notification_time[job_name]).total_seconds()
                            
                              if time_since_notification >= NOTIFICATION_INTERVAL and status in ['Running', 'Queued']:
                                  print(f"\n{'='*70}")
                                  print(f"‚è≥ LONG-RUNNING JOB ALERT")
                                  print(f"{'='*70}")
                                  print(f"Job: {job_name}")
                                  print(f"Status: {status}")
                                  print(f"Running for: {elapsed_hours:.1f} hours")
                                  print(f"Started at: {job_start_time[job_name].strftime('%Y-%m-%d %H:%M:%S')}")
                                
                                # Get job details for notification
                                  job_details = ml_client.jobs.get(job_name)
                                  if hasattr(job_details, 'services') and job_details.services:
                                      studio_url = job_details.services.get('Studio', {}).get('endpoint', '')
                                      if studio_url:
                                          print(f"View in Studio: {studio_url}")
                                
                                  print(f"\nüìß Notification: ML Engineers should check job progress")
                                  print(f"   - Review logs in Azure ML Studio")
                                  print(f"   - Verify compute is running efficiently")
                                  print(f"   - Consider cancelling if stuck")
                                  print(f"{'='*70}\n")
                                
                                # Update notification time
                                  last_notification_time[job_name] = current_time
                                
                                # Log to Azure DevOps timeline (visible in pipeline UI)
                                  print(f"##vso[task.logissue type=warning]Job {job_name} running for {elapsed_hours:.1f}h - Status: {status}")
                            
                            # Check if job completed
                              if status in ['Completed', 'Failed', 'Canceled']:
                                  pending.remove(job_name)
                                
                                  if status == 'Completed':
                                      completed.append(job_name)
                                      print(f"   ‚úÖ {job_name}: Completed (took {elapsed_hours:.1f}h)")
                                  else:
                                      failed.append(job_name)
                                      print(f"   ‚ùå {job_name}: {status} (after {elapsed_hours:.1f}h)")
                              elif status in ['Running', 'Queued']:
                                # Show periodic progress update
                                  if poll_count % 10 == 0:  # Every 10 minutes
                                      print(f"   ‚è≥ {job_name}: {status} ({elapsed_hours:.1f}h elapsed)")
                          except Exception as e:
                              print(f"   ‚ö†Ô∏è  Error checking {job_name}: {e}")
                
                # Save completed jobs for model registration
                  completed_job_map = {}
                  failed_job_map = {}
                
                  for job_info in submitted_jobs:
                      if job_info['job_name'] in completed:
                          key = f"{job_info['plant_id']}_{job_info['circuit_id']}"
                          completed_job_map[key] = job_info
                      elif job_info['job_name'] in failed:
                          key = f"{job_info['plant_id']}_{job_info['circuit_id']}"
                          failed_job_map[key] = job_info
                
                  with open('completed_jobs.json', 'w') as f:
                      json.dump({'completed_jobs': completed_job_map}, f, indent=2)
                
                # Save failed jobs for potential retry
                  with open('failed_jobs.json', 'w') as f:
                      json.dump({'failed_jobs': failed_job_map}, f, indent=2)
                
                # Set pipeline variable to indicate if there are failures
                  has_failures = 'true' if (failed or failed_submissions) else 'false'
                  print(f"##vso[task.setvariable variable=hasTrainingFailures;isOutput=true]{has_failures}")
                
                # Summary
                  print(f"\n{'='*60}")
                  print(f"Training Jobs Summary:")
                  print(f"  ‚úÖ Completed: {len(completed)}")
                  print(f"  ‚ùå Failed/Canceled: {len(failed)}")
                  if failed_submissions:
                      print(f"  ‚ö†Ô∏è  Not Submitted: {len(failed_submissions)}")
                  print(f"{'='*60}\n")
                
                  if failed:
                      print("‚ö†Ô∏è  Failed/canceled jobs:")
                      for job in failed:
                          print(f"   - {job}")
                      print()
                      print(f"üíæ Failed job details saved to failed_jobs.json for retry")
                      print()
                
                  if not completed:
                      print("‚ö†Ô∏è  WARNING: No jobs completed successfully")
                      print("   Pipeline will continue but no models will be registered")
                    # Don't fail - just proceed with empty completed list
                  else:
                      print(f"‚úÖ {len(completed)} job(s) completed successfully")
                    
                      if failed or failed_submissions:
                          total_failures = len(failed) + len(failed_submissions)
                          print(f"‚ö†Ô∏è  Note: {total_failures} job(s) failed/not submitted - continuing with {len(completed)} successful job(s)")
                  EOF
          
          # Publish failed jobs for retry stage
            - publish: 'failed_jobs.json'
              artifact: 'failed_jobs'
              displayName: 'Publish Failed Jobs (for retry)'
              condition: succeededOrFailed()
          
          # ============================================
          # OPTIONAL: Teams Notification (Currently Disabled)
          # ============================================
          # To enable Teams notifications for long-running jobs:
          # 1. Create Teams Workflow webhook (see docs/teams-notification-setup.md)
          # 2. Add pipeline variable: TeamsWebhookURL (mark as secret)
          # 3. Uncomment the task below
          # ============================================
          
          # - task: PowerShell@2
          #   displayName: 'Send Teams Notification (Optional)'
          #   condition: and(succeeded(), ne(variables['TeamsWebhookURL'], ''))
          #   inputs:
          #     targetType: 'inline'
          #     script: |
          #       # This task is currently disabled. See comments above for setup instructions.
          #       Write-Host "‚ÑπÔ∏è  Teams notification disabled - configure TeamsWebhookURL to enable"
      
        - job: RegisterModels
          displayName: 'Register Trained Models'
          dependsOn: WaitForTrainingJobs
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - checkout: self
          
            - task: DownloadPipelineArtifact@2
              inputs:
                artifact: 'changed_circuits'
                path: $(Pipeline.Workspace)
          
            - task: DownloadPipelineArtifact@2
              inputs:
                artifact: 'training_jobs'
                path: $(Pipeline.Workspace)
          
            - task: AzureCLI@2
              displayName: 'Register Models & Capture Versions'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                inlineScript: |
                  echo "üì¶ Registering models and capturing version numbers..."
                
                  python3 << 'EOF'
                  import json
                  import subprocess
                  import sys
                  import yaml
                
                # Load completed jobs mapping
                  with open('$(Pipeline.Workspace)/completed_jobs.json', 'r') as f:
                      data = json.load(f)
                
                  completed_job_map = data.get('completed_jobs', {})
                
                  if not completed_job_map:
                      print("‚ÑπÔ∏è  No completed jobs to register")
                    # Create empty registered models file
                      with open('registered_models.json', 'w') as f:
                          json.dump({'models': []}, f, indent=2)
                      sys.exit(0)
                
                # Load original submission data for context
                  with open('$(Pipeline.Workspace)/training_jobs.json', 'r') as f:
                      submission_data = json.load(f)
                
                  failed_submissions = submission_data.get('failed_submissions', [])
                
                  print(f"üì¶ Registering models for {len(completed_job_map)} completed job(s)...\n")
                
                  if failed_submissions:
                      print(f"‚ÑπÔ∏è  Note: {len(failed_submissions)} circuit(s) failed to submit and will be skipped\n")
                
                  registered_models = []  # List of dicts with model name, version, and metadata
                  failed = []
                
                  for key, job_info in completed_job_map.items():
                      plant_id = job_info['plant_id']
                      circuit_id = job_info['circuit_id']
                      cutoff_date = job_info['cutoff_date']
                      model_name = job_info['model_name']
                      job_name = job_info['job_name']
                    
                      print(f"\nüìä Registering model: {model_name} (cutoff_date: {cutoff_date})")
                      print(f"   From job: {job_name}")
                    
                    # Load circuit config to get config_hash
                      circuit_config_file = f'config/circuits/{plant_id}_{circuit_id}.yaml'
                      with open(circuit_config_file, 'r') as f:
                          circuit_config = yaml.safe_load(f)
                    
                      config_hash = circuit_config.get('metadata', {}).get('config_hash', 'unknown')
                      print(f"   Config hash: {config_hash}")
                    
                    # Get the output model from the job (custom_model type)
                      model_path = f"azureml://jobs/{job_name}/outputs/trained_model"
                    
                      print(f"   Model path: {model_path}")
                    
                    # Check if model with this config_hash already exists (by tag)
                    # NOTE: Same config trained again will get a new version (allowed for retraining scenarios)
                      check_cmd = [
                          'az', 'ml', 'model', 'list',
                          '--name', model_name,
                          '--workspace-name', '$(workspaceName)',
                          '--resource-group', '$(resourceGroup)',
                          '--query', f"[?tags.config_hash=='{config_hash}' && tags.cutoff_date=='{cutoff_date}'] | [0].{{version:version, id:id}}",
                          '-o', 'json'
                      ]
                    
                      check_result = subprocess.run(check_cmd, capture_output=True, text=True)
                    
                      if check_result.returncode == 0 and check_result.stdout.strip() and check_result.stdout.strip() != 'null':
                          existing_model = json.loads(check_result.stdout.strip())
                          existing_version = existing_model['version']
                          print(f"‚úÖ Model {model_name}:v{existing_version} with config_hash={config_hash} and cutoff_date={cutoff_date} already registered.")
                        # Add to registered list for promotion
                          registered_models.append({
                              'model_name': model_name,
                              'version': str(existing_version),
                              'plant_id': plant_id,
                              'circuit_id': circuit_id,
                              'cutoff_date': cutoff_date,
                              'config_hash': config_hash,
                              'training_job': job_name
                          })
                          continue
                    
                    # Register the model (Azure ML will auto-assign version number)
                    # Use custom_model type
                      register_cmd = [
                          'az', 'ml', 'model', 'create',
                          '--name', model_name,
                          '--path', model_path,
                          '--type', 'custom_model',
                          '--workspace-name', '$(workspaceName)',
                          '--resource-group', '$(resourceGroup)',
                          '--set', f'tags.plant_id={plant_id}',
                          '--set', f'tags.circuit_id={circuit_id}',
                          '--set', f'tags.cutoff_date={cutoff_date}',
                          '--set', f'tags.config_hash={config_hash}',
                          '--set', f'tags.training_job={job_name}',
                          '--set', f'description=LSTM model for {plant_id}/{circuit_id} trained with data up to {cutoff_date}',
                          '--query', '{{name:name, version:version}}',
                          '-o', 'json'
                      ]
                    
                      result = subprocess.run(register_cmd, capture_output=True, text=True)
                    
                      if result.returncode != 0:
                          print(f"‚ùå Failed to register {model_name}")
                          print(f"   Error: {result.stderr}")
                          failed.append(f"{plant_id}/{circuit_id}")
                      else:
                        # Parse the returned model info to get auto-assigned version
                          model_info = json.loads(result.stdout.strip())
                          version = str(model_info['version'])
                        
                          print(f"‚úÖ Registered: {model_name}:v{version} (cutoff_date={cutoff_date}, config_hash={config_hash})")
                        
                        # Store model info for promotion stage
                          registered_models.append({
                              'model_name': model_name,
                              'version': version,
                              'plant_id': plant_id,
                              'circuit_id': circuit_id,
                              'cutoff_date': cutoff_date,
                              'config_hash': config_hash,
                              'training_job': job_name
                          })
                
                # Save registered models with versions for promotion stage
                  with open('registered_models.json', 'w') as f:
                      json.dump({'models': registered_models}, f, indent=2)
                
                # Summary
                  print(f"\n{'='*60}")
                  print(f"Model Registration Summary:")
                  print(f"  ‚úÖ Registered: {len(registered_models)}")
                  print(f"  ‚ùå Failed: {len(failed)}")
                  if failed_submissions:
                      print(f"  ‚ö†Ô∏è  Not Attempted (submission failed): {len(failed_submissions)}")
                  print(f"{'='*60}\n")
                
                  if registered_models:
                      print("üìã Registered model versions:")
                      for model in registered_models:
                          print(f"   {model['model_name']}:v{model['version']} (cutoff_date={model['cutoff_date']})")
                      print()
                
                  if failed:
                      print("‚ö†Ô∏è  Failed to register models for (will skip these):")
                      for circuit in failed:
                          print(f"   - {circuit}")
                      print()
                
                  if not registered_models:
                      print("‚ö†Ô∏è  WARNING: No models were successfully registered")
                      print("   This may occur if all training jobs failed or were cancelled")
                      print("   Pipeline will continue to next stage")
                    # Don't fail - just proceed (subsequent stages will have no models to work with)
                  else:
                      print(f"‚úÖ Successfully registered {len(registered_models)} model(s)")
                      print(f"üìù Model versions saved to registered_models.json for promotion")
                    
                      if failed or failed_submissions:
                          total_issues = len(failed) + len(failed_submissions)
                          print(f"‚ö†Ô∏è  Note: {total_issues} circuit(s) had issues - continuing with {len(registered_models)} successful registration(s)")
                  EOF
          
            - task: PublishPipelineArtifact@1
              displayName: 'Publish Registered Model Versions'
              inputs:
                targetPath: 'registered_models.json'
                artifact: 'registered_models'
  
    - stage: RetryFailedTraining
      displayName: '[DEV] Retry Failed Training (Approval Required)'
      dependsOn: TrainModels
      condition: |
        and(
          ne(variables['Build.Reason'], 'PullRequest'),
          eq(dependencies.TrainModels.outputs['WaitForTrainingJobs.monitorJobs.hasTrainingFailures'], 'true')
        )
      jobs:
        - deployment: RetryApprovalGate
          displayName: 'Approve Retry for Failed Circuits'
          environment: 'training-retry'
          pool:
            vmImage: 'ubuntu-latest'
          strategy:
            runOnce:
              deploy:
                steps:
                  - checkout: self
                
                  - task: DownloadPipelineArtifact@2
                    inputs:
                      artifact: 'failed_jobs'
                      path: $(Pipeline.Workspace)
                
                  - script: |
                      echo "‚ö†Ô∏è  Training failures detected"
                      echo ""
                      echo "Failed circuits:"
                      python3 << 'EOF'
                        import json

                        with open('$(Pipeline.Workspace)/failed_jobs.json', 'r') as f:
                            data = json.load(f)

                        failed_jobs = data.get('failed_jobs', {})

                        if not failed_jobs:
                            print("  (No failed jobs found)")
                        else:
                            for key, job_info in failed_jobs.items():
                                plant_id = job_info['plant_id']
                                circuit_id = job_info['circuit_id']
                                job_name = job_info['job_name']
                                print(f"  - {plant_id}/{circuit_id} (job: {job_name})")
                        EOF
                      echo ""
                      echo "‚ÑπÔ∏è  Manual approval required to retry these circuits"
                      echo "   Review job logs in Azure ML Studio before approving"
                    displayName: 'Show Failed Circuits'
                
                  - task: AzureCLI@2
                    displayName: 'Retry Failed Training Jobs'
                    inputs:
                      azureSubscription: '$(azureServiceConnection)'
                      scriptType: 'bash'
                      scriptLocation: 'inlineScript'
                      inlineScript: |
                        echo "‚úÖ Manual approval received for retry"
                        echo "üîÑ Retrying failed training jobs..."
                        echo ""
                      
                        python3 << 'EOF'
                          import json
                          import subprocess
                          import sys
                          import datetime

                          with open('$(Pipeline.Workspace)/failed_jobs.json', 'r') as f:
                              data = json.load(f)

                          failed_jobs = data.get('failed_jobs', {})

                          if not failed_jobs:
                              print("‚ÑπÔ∏è  No failed jobs to retry")
                              sys.exit(0)

                          print(f"üîÑ Retrying {len(failed_jobs)} failed circuit(s)...\n")

                          retried_jobs = []
                          retry_failures = []

                          for key, job_info in failed_jobs.items():
                              plant_id = job_info['plant_id']
                              circuit_id = job_info['circuit_id']
                              cutoff_date = job_info['cutoff_date']
                            
                              print(f"\nüìä Retrying: {plant_id}_{circuit_id}")
                            
                              circuit_config = f'config/circuits/{plant_id}_{circuit_id}.yaml'
                              data_name = f"{plant_id}_{circuit_id}"
                              data_version = cutoff_date
                              mltable_uri = f"azureml:{data_name}:{data_version}"
                            
                            # New job name with retry suffix
                              timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
                              job_name = f"{plant_id}_{circuit_id}_{cutoff_date}_retry_{timestamp}".replace('-', '_').replace(':', '_')
                            
                              print(f"   Config: {circuit_config}")
                              print(f"   Data: {mltable_uri}")
                              print(f"   Job name: {job_name}")
                            
                              cmd = [
                                  'az', 'ml', 'job', 'create',
                                  '--file', 'pipelines/single-circuit-training.yaml',
                                  '--name', job_name,
                                  '--set', f'inputs.circuit_config.path={circuit_config}',
                                  '--set', f'inputs.training_data.path={mltable_uri}',
                                  '--workspace-name', '$(workspaceName)',
                                  '--resource-group', '$(resourceGroup)',
                                  '--query', 'name',
                                  '-o', 'tsv'
                              ]
                            
                              result = subprocess.run(cmd, capture_output=True, text=True)
                            
                              if result.returncode == 0:
                                  job_name_out = result.stdout.strip()
                                  retried_jobs.append({
                                      'job_name': job_name_out,
                                      'plant_id': plant_id,
                                      'circuit_id': circuit_id,
                                      'cutoff_date': cutoff_date,
                                      'model_name': job_info.get('model_name', f'{plant_id.lower()}-{circuit_id.lower()}')
                                  })
                                  print(f"   ‚úÖ Submitted: {job_name_out}")
                              else:
                                  retry_failures.append(f"{plant_id}/{circuit_id}")
                                  print(f"   ‚ùå Failed to submit: {result.stderr[:100]}")

                        # Save retried jobs
                          with open('retry_jobs.json', 'w') as f:
                              json.dump({'retried_jobs': retried_jobs, 'retry_failures': retry_failures}, f, indent=2)

                          print(f"\n{'='*60}")
                          print(f"Retry Summary:")
                          print(f"  ‚úÖ Submitted: {len(retried_jobs)}")
                          print(f"  ‚ùå Failed to submit: {len(retry_failures)}")
                          print(f"{'='*60}\n")

                          if retry_failures:
                              print("‚ö†Ô∏è  Failed to retry:")
                              for circuit in retry_failures:
                                  print(f"   - {circuit}")
                              print()

                          if not retried_jobs:
                              print("‚ùå No retry jobs submitted successfully")
                              sys.exit(1)

                          print(f"‚úÖ {len(retried_jobs)} retry job(s) submitted")
                          print("üìù Retry jobs will be monitored in the next job")
                          EOF
                
                  - publish: 'retry_jobs.json'
                    artifact: 'retry_jobs'
                    displayName: 'Publish Retry Jobs'
      
        - job: WaitForRetryJobs
          displayName: 'Wait for Retry Jobs'
          dependsOn: RetryApprovalGate
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - task: DownloadPipelineArtifact@2
              inputs:
                artifact: 'retry_jobs'
                path: $(Pipeline.Workspace)
          
            - task: AzureCLI@2
              displayName: 'Monitor Retry Jobs'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                addSpnToEnvironment: true
                inlineScript: |
                  echo "‚è≥ Waiting for retry jobs to complete..."
                
                  pip install -q azure-ai-ml azure-identity
                
                  python3 << 'EOF'
                    import json
                    import time
                    import sys
                    from azure.identity import DefaultAzureCredential
                    from azure.ai.ml import MLClient

                    with open('$(Pipeline.Workspace)/retry_jobs.json', 'r') as f:
                        data = json.load(f)

                    retried_jobs = data.get('retried_jobs', [])

                    if not retried_jobs:
                        print("‚ÑπÔ∏è  No retry jobs to monitor")
                        sys.exit(0)

                    job_names = [job['job_name'] for job in retried_jobs]

                    print(f"üìä Monitoring {len(job_names)} retry job(s)...\n")

                    credential = DefaultAzureCredential()
                    ml_client = MLClient(
                        credential=credential,
                        subscription_id='$(subscriptionId)',
                        resource_group_name='$(resourceGroup)',
                        workspace_name='$(workspaceName)'
                    )

                    pending = set(job_names)
                    completed = []
                    failed = []

                    while pending:
                        time.sleep(60)
                      
                        for job_name in list(pending):
                            try:
                                job = ml_client.jobs.get(job_name)
                                status = job.status
                              
                                if status in ['Completed', 'Failed', 'Canceled']:
                                    pending.remove(job_name)
                                  
                                    if status == 'Completed':
                                        completed.append(job_name)
                                        print(f"‚úÖ {job_name}: Completed")
                                    else:
                                        failed.append(job_name)
                                        print(f"‚ùå {job_name}: {status}")
                            except Exception as e:
                                print(f"‚ö†Ô∏è  Error checking {job_name}: {e}")

                  # Save completed retry jobs
                    completed_retry_map = {}
                    for job_info in retried_jobs:
                        if job_info['job_name'] in completed:
                            key = f"{job_info['plant_id']}_{job_info['circuit_id']}"
                            completed_retry_map[key] = job_info

                    with open('completed_retry_jobs.json', 'w') as f:
                        json.dump({'completed_jobs': completed_retry_map}, f, indent=2)

                    print(f"\n{'='*60}")
                    print(f"Retry Results:")
                    print(f"  ‚úÖ Completed: {len(completed)}")
                    print(f"  ‚ùå Failed: {len(failed)}")
                    print(f"{'='*60}\n")

                    if failed:
                        print("‚ö†Ô∏è  Retry jobs that still failed:")
                        for job in failed:
                            print(f"   - {job}")
                        print()

                    if completed:
                        print(f"‚úÖ {len(completed)} retry job(s) completed successfully")
                    else:
                        print("‚ö†Ô∏è  No retry jobs completed successfully")
                    EOF
          
            - publish: 'completed_retry_jobs.json'
              artifact: 'completed_retry_jobs'
              displayName: 'Publish Completed Retry Jobs'
      
        - job: RegisterRetryModels
          displayName: 'Register Retry Models'
          dependsOn: WaitForRetryJobs
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - checkout: self
          
            - task: DownloadPipelineArtifact@2
              inputs:
                artifact: 'completed_retry_jobs'
                path: $(Pipeline.Workspace)
          
            - task: DownloadPipelineArtifact@2
              inputs:
                artifact: 'registered_models'
                path: $(Pipeline.Workspace)
          
            - task: AzureCLI@2
              displayName: 'Register Retry Models & Merge with Original'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                inlineScript: |
                  echo "üì¶ Registering models from retry jobs..."
                
                  python3 << 'EOF'
                    import json
                    import subprocess
                    import sys
                    import yaml

                    with open('$(Pipeline.Workspace)/completed_retry_jobs.json', 'r') as f:
                        retry_data = json.load(f)

                    completed_retry_map = retry_data.get('completed_jobs', {})

                    if not completed_retry_map:
                        print("‚ÑπÔ∏è  No completed retry jobs to register")
                        sys.exit(0)

                    print(f"üì¶ Registering {len(completed_retry_map)} retry model(s)...\n")

                    retry_registered = []

                    for key, job_info in completed_retry_map.items():
                        plant_id = job_info['plant_id']
                        circuit_id = job_info['circuit_id']
                        cutoff_date = job_info['cutoff_date']
                        model_name = job_info['model_name']
                        job_name = job_info['job_name']
                      
                        print(f"\nüìä Registering: {model_name}")
                      
                        circuit_config_file = f'config/circuits/{plant_id}_{circuit_id}.yaml'
                        with open(circuit_config_file, 'r') as f:
                            circuit_config = yaml.safe_load(f)
                      
                        config_hash = circuit_config.get('metadata', {}).get('config_hash', 'unknown')
                        model_path = f"azureml://jobs/{job_name}/outputs/trained_model"
                      
                        cmd = [
                            'az', 'ml', 'model', 'create',
                            '--name', model_name,
                            '--path', model_path,
                            '--type', 'custom_model',
                            '--set', f'tags.plant_id={plant_id}',
                            '--set', f'tags.circuit_id={circuit_id}',
                            '--set', f'tags.cutoff_date={cutoff_date}',
                            '--set', f'tags.config_hash={config_hash}',
                            '--set', f'tags.training_job={job_name}',
                            '--set', 'tags.retry=true',
                            '--workspace-name', '$(workspaceName)',
                            '--resource-group', '$(resourceGroup)',
                            '-o', 'json'
                        ]
                      
                        result = subprocess.run(cmd, capture_output=True, text=True)
                      
                        if result.returncode == 0:
                            model_info = json.loads(result.stdout.strip())
                            version = str(model_info['version'])
                          
                            retry_registered.append({
                                'model_name': model_name,
                                'version': version,
                                'plant_id': plant_id,
                                'circuit_id': circuit_id,
                                'cutoff_date': cutoff_date,
                                'config_hash': config_hash,
                                'training_job': job_name,
                                'retry': True
                            })
                          
                            print(f"‚úÖ Registered: {model_name}:v{version} (retry)")

                  # Merge with original registered models
                    with open('$(Pipeline.Workspace)/registered_models.json', 'r') as f:
                        original_data = json.load(f)

                    original_models = original_data.get('models', [])
                    all_models = original_models + retry_registered

                    with open('registered_models.json', 'w') as f:
                        json.dump({'models': all_models}, f, indent=2)

                    print(f"\n{'='*60}")
                    print(f"Retry Model Registration:")
                    print(f"  ‚úÖ Registered from retry: {len(retry_registered)}")
                    print(f"  üìä Total models (original + retry): {len(all_models)}")
                    print(f"{'='*60}\n")

                    if retry_registered:
                        print("‚úÖ Retry models successfully registered and merged")
                    else:
                        print("‚ö†Ô∏è  No retry models were registered")
                    EOF
          
            - task: PublishPipelineArtifact@1
              displayName: 'Publish Updated Model List'
              inputs:
                targetPath: 'registered_models.json'
                artifact: 'registered_models_with_retry'
  
    - stage: PromoteToRegistry
      displayName: '[DEV] Promote to Registry (Approval Required)'
      dependsOn:
        - TrainModels
        - RetryFailedTraining
      condition: |
        and(
          ne(variables['Build.Reason'], 'PullRequest'),
          in(dependencies.TrainModels.result, 'Succeeded', 'SucceededWithIssues'),
          in(dependencies.RetryFailedTraining.result, 'Succeeded', 'Skipped')
        )
      jobs:
        - deployment: ApprovalGate
          displayName: 'ML Engineer Approval'
          environment: 'registry-promotion'
          pool:
            vmImage: 'ubuntu-latest'
          strategy:
            runOnce:
              deploy:
                steps:
                  - checkout: self
                
                  - task: DownloadPipelineArtifact@2
                    displayName: 'Download Original Registered Models'
                    inputs:
                      buildType: 'current'
                      artifactName: 'registered_models'
                      targetPath: $(Pipeline.Workspace)/registered_models
                    continueOnError: true
                
                  - task: DownloadPipelineArtifact@2
                    displayName: 'Download Registered Models (with retry)'
                    inputs:
                      buildType: 'current'
                      artifactName: 'registered_models_with_retry'
                      targetPath: $(Pipeline.Workspace)/registered_models_with_retry
                    continueOnError: true
                
                  - script: |
                    # Use retry version if available, otherwise use original
                      if [ -f "$(Pipeline.Workspace)/registered_models_with_retry/registered_models.json" ]; then
                        echo "üì¶ Using merged model list (includes retry models)"
                        cp $(Pipeline.Workspace)/registered_models_with_retry/registered_models.json $(Pipeline.Workspace)/registered_models.json
                      elif [ -f "$(Pipeline.Workspace)/registered_models/registered_models.json" ]; then
                        echo "üì¶ Using original model list (no retry)"
                        cp $(Pipeline.Workspace)/registered_models/registered_models.json $(Pipeline.Workspace)/registered_models.json
                      else
                        echo "‚ùå No registered models found"
                        exit 1
                      fi
                    displayName: 'Select Model List'
                
                  - task: DownloadPipelineArtifact@2
                    inputs:
                      artifact: 'environment_info'
                      path: $(Pipeline.Workspace)
                
                  - task: AzureCLI@2
                    displayName: 'Promote Environment & Models to Registry'
                    inputs:
                      azureSubscription: '$(azureServiceConnection)'
                      scriptType: 'bash'
                      scriptLocation: 'inlineScript'
                      inlineScript: |
                        echo "‚úÖ Manual approval received"
                        echo "üèõÔ∏è Promoting infrastructure and models to Registry..."
                        echo ""
                      
                      # Check if environment needs promotion
                        echo "üì¶ Checking environment promotion..."
                        python3 << 'PYEOF'
                        import json
                        import subprocess
                        import sys

                      # Load environment info from build stage
                        with open('$(Pipeline.Workspace)/environment_info.json', 'r') as f:
                            env_info = json.load(f)

                        env_name = env_info['name']
                        env_version = env_info['version']
                        newly_created = env_info.get('newly_created', False)

                        print("Environment: " + env_name + ":" + env_version)
                        print("Newly created in Dev: " + str(newly_created))
                        print()

                      # Always check if environment exists in Registry
                        print("Checking Registry for environment " + env_name + ":" + env_version + "...")
                        check_cmd = [
                            'az', 'ml', 'environment', 'show',
                            '--name', env_name,
                            '--version', env_version,
                            '--registry-name', '$(registryName)',
                            '--resource-group', '$(registryResourceGroup)'
                        ]

                        result = subprocess.run(check_cmd, capture_output=True, text=True)

                        if result.returncode == 0:
                            print("‚úÖ Environment " + env_name + ":" + env_version + " already exists in Registry. Skipping.")
                        else:
                          # Environment not in Registry - promote it
                            if newly_created:
                                print("üÜï Environment was newly created in Dev workspace")
                            else:
                                print("‚ö†Ô∏è  Environment exists in Dev but not in Registry")
                          
                            print("   Promoting to Registry...")
                          
                            promote_cmd = [
                                'az', 'ml', 'environment', 'create',
                                '--file', 'config/environment.yaml',
                                '--registry-name', '$(registryName)',
                                '--resource-group', '$(registryResourceGroup)'
                            ]
                          
                            promote_result = subprocess.run(promote_cmd, capture_output=True, text=True)
                          
                            if promote_result.returncode != 0:
                                print("‚ùå Failed to promote environment to Registry")
                                print("   Error: " + promote_result.stderr)
                                sys.exit(1)
                            else:
                                print("‚úÖ Promoted environment " + env_name + ":" + env_version + " to Registry")

                        print()
                        PYEOF
                      
                        echo ""
                      
                      # Promote trained models to registry using captured version numbers
                        echo "ü§ñ Promoting trained models to Registry..."
                        python3 << 'MODEOF'
                        import json
                        import subprocess
                        import sys
                        import os
                      
                      # Validate artifact exists
                        artifact_path = '$(Pipeline.Workspace)/registered_models.json'
                        if not os.path.exists(artifact_path):
                            print("‚ö†Ô∏è  WARNING: registered_models.json artifact not found")
                            print("   This indicates all training jobs may have failed")
                            print("   Skipping model promotion")
                            sys.exit(0)
                      
                      # Load registered models with version numbers from Dev workspace
                        with open(artifact_path, 'r') as f:
                            data = json.load(f)
                      
                        registered_models = data.get('models', [])
                      
                        if not registered_models:
                            print("‚ÑπÔ∏è  No models to promote")
                            print("   This may occur if all training jobs failed or were cancelled")
                            sys.exit(0)
                      
                        print("üìä Promoting " + str(len(registered_models)) + " model(s) to Registry...")
                        print()
                      
                        promoted = []
                        failed = []
                      
                        for model_info in registered_models:
                            model_name = model_info['model_name']
                            model_version = str(model_info['version'])
                            plant_id = model_info['plant_id']
                            circuit_id = model_info['circuit_id']
                            cutoff_date = model_info['cutoff_date']
                            config_hash = model_info.get('config_hash', 'unknown')
                            training_job = model_info['training_job']
                          
                            print("\nüîÑ Promoting: " + model_name + ":v" + model_version + " (config_hash: " + config_hash + ", cutoff_date: " + cutoff_date + ")")
                          
                          # Check if model already exists in registry with this config_hash and version
                            check_cmd = [
                                'az', 'ml', 'model', 'list',
                                '--name', model_name,
                                '--registry-name', '$(registryName)',
                                '--resource-group', '$(registryResourceGroup)',
                                '--query', "[?tags.config_hash=='" + config_hash + "' && version=='" + model_version + "'].version",
                                '-o', 'tsv'
                            ]
                          
                            check_result = subprocess.run(check_cmd, capture_output=True, text=True)
                          
                            if check_result.returncode == 0 and check_result.stdout.strip():
                                existing_version = check_result.stdout.strip()
                                print("‚úÖ Model " + model_name + ":v" + existing_version + " with config_hash=" + config_hash + " already exists in Registry. Skipping.")
                                promoted.append(model_name + ":v" + existing_version)
                                continue
                          
                          # Share model from Dev workspace to Registry using az ml model share
                            print("   Sharing model to Registry...")
                            share_cmd = [
                                'az', 'ml', 'model', 'share',
                                '--name', model_name,
                                '--version', model_version,
                                '--workspace-name', '$(workspaceName)',
                                '--resource-group', '$(resourceGroup)',
                                '--registry-name', '$(registryName)',
                                '--share-with-name', model_name,
                                '--share-with-version', model_version
                            ]
                          
                            result = subprocess.run(share_cmd, capture_output=True, text=True)
                          
                            if result.returncode != 0:
                                print("‚ùå Failed to share " + model_name + ":v" + model_version + " to Registry")
                                print("   Error: " + result.stderr)
                                failed.append(model_name + ":v" + model_version)
                            else:
                              # Model shared successfully - it keeps the same version in Registry
                                print("‚úÖ Shared to Registry as v" + model_version)
                                promoted.append(model_name + ":v" + model_version)
                      
                      # Summary
                        print("\n" + "="*60)
                        print("Registry Promotion Summary:")
                        print("  ‚úÖ Promoted: " + str(len(promoted)))
                        print("  ‚ùå Failed: " + str(len(failed)))
                        print("="*60 + "\n")
                      
                        if promoted:
                            print("üìã Promoted models:")
                            for model in promoted:
                                print("   " + model)
                            print()
                      
                        if failed:
                            print("‚ö†Ô∏è  Failed to promote:")
                            for model in failed:
                                print("   " + model)
                            print("\n‚ùå Some models failed to promote")
                            sys.exit(1)
                      
                        print("‚úÖ All models promoted to Registry successfully")
                        MODEOF

  # ============================================
  # RELEASE/* BRANCH: Deploy from Registry to Test
  # ============================================
  - ${{ if startsWith(variables['Build.SourceBranch'], 'refs/heads/release/') }}:
  
      - stage: VerifyRegistry
      displayName: '[TEST] Verify Registry Models'
      condition: ne(variables['Build.Reason'], 'PullRequest')  # Skip on PRs
      jobs:
        - job: QueryRegistry
          displayName: 'Query Latest Models from Registry'
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - checkout: self
          
            - script: python3 scripts/generate_circuit_configs.py
              displayName: 'Generate Circuit Configs'
          
            - task: AzureCLI@2
              displayName: 'Query Registry for Production-Ready Models'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                inlineScript: |
                  echo "üèõÔ∏è Querying Registry for models by config hash..."
                
                  python3 << 'EOF'
                  import json
                  import subprocess
                  import sys
                  import yaml
                  import os
                  from pathlib import Path
                
                # Load individual circuit config files (generated earlier) to get config hashes
                  config_dir = Path('config/circuits')
                
                  if not config_dir.exists():
                      print("‚ùå Circuit configs directory not found")
                      sys.exit(1)
                
                  config_files = sorted(config_dir.glob('*.yaml'))
                
                  if not config_files:
                      print("‚ùå No circuit config files found")
                      sys.exit(1)
                
                  print(f"üìÇ Found {len(config_files)} circuit config file(s)")
                  print()
                
                  models_info = []
                  not_found = []
                
                  for config_file in config_files:
                    # Load circuit config
                      with open(config_file, 'r') as f:
                          circuit = yaml.safe_load(f)
                    
                      plant_id = circuit['plant_id']
                      circuit_id = circuit['circuit_id']
                      cutoff_date = circuit.get('cutoff_date', '')
                      model_name = circuit.get('model_name', f'{plant_id.lower()}-{circuit_id.lower()}')
                    
                    # Get config hash from metadata
                      config_hash = circuit.get('metadata', {}).get('config_hash', 'unknown')
                    
                      print(f"\nüîç Checking: {model_name} (config_hash: {config_hash}, cutoff_date: {cutoff_date})")
                    
                    # Query Registry by config_hash tag (this is the key change!)
                    # This ensures we only find models matching the CURRENT configuration
                      check_cmd = [
                          'az', 'ml', 'model', 'list',
                          '--name', model_name,
                          '--registry-name', '$(registryName)',
                          '--resource-group', '$(registryResourceGroup)',
                          '--query', f"[?tags.config_hash=='{config_hash}'] | [0]",
                          '-o', 'json'
                      ]
                    
                      result = subprocess.run(check_cmd, capture_output=True, text=True)
                    
                      if result.returncode == 0 and result.stdout.strip() and result.stdout.strip() != 'null':
                          model = json.loads(result.stdout)
                          models_info.append({
                              'plant_id': plant_id,
                              'circuit_id': circuit_id,
                              'model_name': model_name,
                              'version': str(model['version']),
                              'cutoff_date': cutoff_date,
                              'config_hash': config_hash,
                              'tags': model.get('tags', {}),
                              'description': circuit.get('description', '')
                          })
                          print(f"‚úÖ Found: {model_name}:v{model['version']} (config_hash={config_hash})")
                      else:
                          not_found.append(f"{model_name} (config_hash={config_hash})")
                          print(f"‚ö†Ô∏è  Not found in Registry: {model_name} with config_hash={config_hash}")
                
                  if not_found:
                      print(f"\n‚ö†Ô∏è  Warning: {len(not_found)} model(s) not found in Registry:")
                      for model in not_found:
                          print(f"   - {model}")
                      print("\n‚ÑπÔ∏è  This is expected for circuits with new/changed configurations that haven't been trained yet.")
                      print("   Only models matching the current config_hash will be deployed.")
                      print("   Proceeding with available models only.")
                
                  if not models_info:
                      print("\n‚ùå No models found in Registry. Cannot proceed with deployment.")
                      sys.exit(1)
                
                # Save models info
                  with open('registry_models.json', 'w') as f:
                      json.dump({'models': models_info}, f, indent=2)
                
                  print(f"\n‚úÖ Verified {len(models_info)} model(s) available in Registry for deployment")
                  EOF
          
            - task: PublishPipelineArtifact@1
              inputs:
                targetPath: 'registry_models.json'
                artifact: 'registry_models'
  
    - stage: DeployToTest
      displayName: '[TEST] Deploy to Test Endpoints (Approval Required)'
      dependsOn: VerifyRegistry
      condition: ne(variables['Build.Reason'], 'PullRequest')  # Skip on PRs
      jobs:
        - deployment: DeployTest
          displayName: 'QA Lead + ML Lead Approval'
          environment: 'test-deployment'
          pool:
            vmImage: 'ubuntu-latest'
          strategy:
            runOnce:
              deploy:
                steps:
                  - checkout: self
                
                  - task: DownloadPipelineArtifact@2
                    inputs:
                      artifact: 'registry_models'
                      path: $(Pipeline.Workspace)
                
                  - task: AzureCLI@2
                    displayName: 'Deploy Batch Endpoints from Registry'
                    inputs:
                      azureSubscription: '$(azureServiceConnection)'
                      scriptType: 'bash'
                      scriptLocation: 'inlineScript'
                      inlineScript: |
                        echo "‚úÖ Deployment approved"
                        echo "üöÄ Deploying batch endpoints to Test workspace..."
                        echo ""
                      
                        python3 << 'EOF'
                        import json
                        import subprocess
                        import sys
                        import time
                      
                        with open('$(Pipeline.Workspace)/registry_models.json', 'r') as f:
                            data = json.load(f)
                      
                        models = data['models']
                        failed = False
                      
                        for model in models:
                            plant_id = model['plant_id']
                            circuit_id = model['circuit_id']
                            model_name = model['model_name']
                            version = model['version']
                          
                          # Endpoint and deployment names (lowercase, no underscores)
                            endpoint_name = f"{plant_id.lower()}-{circuit_id.lower()}-test".replace('_', '-')
                            deployment_name = f"deployment-{version}".replace('.', '-').replace('_', '-')
                          
                            print(f"\n{'='*60}")
                            print(f"Deploying: {endpoint_name}")
                            print(f"Model: azureml://registries/$(registryName)/models/{model_name}/versions/{version}")
                            print(f"{'='*60}")
                          
                          # Step 1: Create/update batch endpoint
                            print(f"\n1. Creating batch endpoint: {endpoint_name}")
                          
                            check_endpoint = subprocess.run([
                                'az', 'ml', 'batch-endpoint', 'show',
                                '--name', endpoint_name,
                                '--workspace-name', '$(workspaceName)',
                                '--resource-group', '$(resourceGroup)'
                            ], capture_output=True, text=True)
                          
                            if check_endpoint.returncode != 0:
                              # Create new endpoint
                                create_endpoint = subprocess.run([
                                    'az', 'ml', 'batch-endpoint', 'create',
                                    '--name', endpoint_name,
                                    '--workspace-name', '$(workspaceName)',
                                    '--resource-group', '$(resourceGroup)'
                                ], capture_output=True, text=True)
                              
                                if create_endpoint.returncode != 0:
                                    print(f"‚ùå Failed to create endpoint: {create_endpoint.stderr}")
                                    failed = True
                                    continue
                                else:
                                    print(f"‚úÖ Endpoint created: {endpoint_name}")
                            else:
                                print(f"‚úÖ Endpoint already exists: {endpoint_name}")
                          
                          # Step 2: Create deployment with model from Registry
                            print(f"\n2. Creating deployment: {deployment_name}")
                          
                            model_path = f"azureml://registries/$(registryName)/models/{model_name}/versions/{version}"
                          
                            create_deployment = subprocess.run([
                                'az', 'ml', 'batch-deployment', 'create',
                                '--name', deployment_name,
                                '--endpoint-name', endpoint_name,
                                '--model', model_path,
                                '--compute', '$(testScoringCluster)',
                                '--instance-count', '1',
                                '--max-concurrency-per-instance', '2',
                                '--mini-batch-size', '10',
                                '--output-action', 'append_row',
                                '--output-file-name', 'predictions.csv',
                                '--workspace-name', '$(workspaceName)',
                                '--resource-group', '$(resourceGroup)'
                            ], capture_output=True, text=True)
                          
                            if create_deployment.returncode != 0:
                                print(f"‚ùå Failed to create deployment: {create_deployment.stderr}")
                                failed = True
                                continue
                            else:
                                print(f"‚úÖ Deployment created: {deployment_name}")
                          
                          # Step 3: Set as default deployment
                            print(f"\n3. Setting default deployment...")
                          
                            set_default = subprocess.run([
                                'az', 'ml', 'batch-endpoint', 'update',
                                '--name', endpoint_name,
                                '--set', f'defaults.deployment_name={deployment_name}',
                                '--workspace-name', '$(workspaceName)',
                                '--resource-group', '$(resourceGroup)'
                            ], capture_output=True, text=True)
                          
                            if set_default.returncode != 0:
                                print(f"‚ö†Ô∏è  Warning: Could not set default deployment")
                            else:
                                print(f"‚úÖ Default deployment set: {deployment_name}")
                          
                            print(f"\n‚úÖ Successfully deployed: {endpoint_name}")
                            time.sleep(2)  # Brief pause between deployments
                      
                        if failed:
                            print("\n‚ùå Some deployments failed")
                            sys.exit(1)
                      
                        print(f"\n{'='*60}")
                        print(f"‚úÖ All Test batch endpoints deployed successfully")
                        print(f"{'='*60}")
                        EOF
  
    - stage: IntegrationTests
      displayName: '[TEST] Integration Tests'
      dependsOn: DeployToTest
      condition: ne(variables['Build.Reason'], 'PullRequest')  # Skip on PRs
      jobs:
        - job: RunTests
          displayName: 'Run Smoke Tests'
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - script: |
                echo "üß™ Running integration tests on Test endpoints..."
                echo "‚úÖ Tests passed"
              displayName: 'Run Tests'
  
    - stage: QASignOff
      displayName: '[TEST] QA Sign-Off (Approval Required)'
      dependsOn: IntegrationTests
      condition: ne(variables['Build.Reason'], 'PullRequest')  # Skip on PRs
      jobs:
        - deployment: QAApproval
          displayName: 'QA Team + Product Owner Approval'
          environment: 'qa-signoff'
          pool:
            vmImage: 'ubuntu-latest'
          strategy:
            runOnce:
              deploy:
                steps:
                  - task: DownloadPipelineArtifact@2
                    inputs:
                      artifact: 'registry_models'
                      path: $(Pipeline.Workspace)
                
                  - task: AzureCLI@2
                    displayName: 'Tag Models as Production-Ready'
                    inputs:
                      azureSubscription: '$(azureServiceConnection)'
                      scriptType: 'bash'
                      scriptLocation: 'inlineScript'
                      inlineScript: |
                        echo "‚úÖ QA approval received"
                        echo "üè∑Ô∏è Tagging models as 'production-ready' in Registry..."
                      
                        python3 << 'EOF'
                        import json
                        import subprocess
                        import sys
                      
                      # Load models from test deployment
                        with open('$(Pipeline.Workspace)/registry_models.json', 'r') as f:
                            data = json.load(f)
                      
                        models = data.get('models', [])
                      
                        if not models:
                            print("‚ö†Ô∏è  No models to tag")
                            sys.exit(0)
                      
                        print(f"üè∑Ô∏è Tagging {len(models)} model(s) as production-ready...")
                        print()
                      
                        failed = False
                        for model in models:
                            model_name = model['model_name']
                            version = model['version']
                            config_hash = model.get('config_hash', 'unknown')
                          
                            print(f"üè∑Ô∏è Tagging: {model_name}:v{version} (config_hash={config_hash})")
                          
                            tag_cmd = [
                                'az', 'ml', 'model', 'update',
                                '--name', model_name,
                                '--version', version,
                                '--registry-name', '$(registryName)',
                                '--resource-group', '$(registryResourceGroup)',
                                '--set', 'tags.production_ready=true'
                            ]
                          
                            result = subprocess.run(tag_cmd, capture_output=True, text=True)
                          
                            if result.returncode != 0:
                                print(f"‚ùå Failed to tag {model_name}:v{version}")
                                print(f"   Error: {result.stderr}")
                                failed = True
                            else:
                                print(f"‚úÖ Tagged: {model_name}:v{version}")
                      
                        if failed:
                            print("\n‚ùå Some models failed to be tagged")
                            sys.exit(1)
                      
                        print(f"\n‚úÖ All {len(models)} model(s) tagged as production-ready")
                        EOF

  # ============================================
  # MAIN BRANCH: Deploy from Registry to Production
  # ============================================
  - ${{ if eq(variables['Build.SourceBranch'], 'refs/heads/main') }}:
  
      - stage: VerifyRegistryProd
      displayName: '[PROD] Verify Production-Ready Models'
      condition: ne(variables['Build.Reason'], 'PullRequest')  # Skip on PRs
      jobs:
        - job: QueryRegistryProd
          displayName: 'Query Production-Ready Models from Registry'
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - checkout: self
          
            - script: python3 scripts/generate_circuit_configs.py
              displayName: 'Generate Circuit Configs'
          
            - task: AzureCLI@2
              displayName: 'Query Registry for Production-Ready Models'
              inputs:
                azureSubscription: '$(azureServiceConnection)'
                scriptType: 'bash'
                scriptLocation: 'inlineScript'
                inlineScript: |
                  echo "üèõÔ∏è Querying Registry for production-ready models by config hash..."
                
                  python3 << 'EOF'
                  import json
                  import subprocess
                  import sys
                  import yaml
                  import os
                  from pathlib import Path
                
                # Load individual circuit config files to get config hashes
                  config_dir = Path('config/circuits')
                
                  if not config_dir.exists():
                      print("‚ùå Circuit configs directory not found")
                      sys.exit(1)
                
                  config_files = sorted(config_dir.glob('*.yaml'))
                
                  if not config_files:
                      print("‚ùå No circuit config files found")
                      sys.exit(1)
                
                  print(f"üìÇ Found {len(config_files)} circuit config file(s)")
                  print()
                
                  models_info = []
                  not_found = []
                
                  for config_file in config_files:
                    # Load circuit config
                      with open(config_file, 'r') as f:
                          circuit = yaml.safe_load(f)
                    
                      plant_id = circuit['plant_id']
                      circuit_id = circuit['circuit_id']
                      cutoff_date = circuit.get('cutoff_date', '')
                      model_name = circuit.get('model_name', f'{plant_id.lower()}-{circuit_id.lower()}')
                    
                    # Get config hash from metadata
                      config_hash = circuit.get('metadata', {}).get('config_hash', 'unknown')
                    
                      print(f"\nüîç Checking: {model_name} (config_hash: {config_hash})")
                    
                    # Query Registry by config_hash AND production-ready tag
                      check_cmd = [
                          'az', 'ml', 'model', 'list',
                          '--name', model_name,
                          '--registry-name', '$(registryName)',
                          '--resource-group', '$(registryResourceGroup)',
                          '--query', f"[?tags.config_hash=='{config_hash}' && tags.production_ready=='true'] | [0]",
                          '-o', 'json'
                      ]
                    
                      result = subprocess.run(check_cmd, capture_output=True, text=True)
                    
                      if result.returncode == 0 and result.stdout.strip() and result.stdout.strip() != 'null':
                          model = json.loads(result.stdout)
                          models_info.append({
                              'plant_id': plant_id,
                              'circuit_id': circuit_id,
                              'model_name': model_name,
                              'version': str(model['version']),
                              'cutoff_date': cutoff_date,
                              'config_hash': config_hash,
                              'tags': model.get('tags', {}),
                              'description': circuit.get('description', '')
                          })
                          print(f"‚úÖ Found: {model_name}:v{model['version']} (config_hash={config_hash}, production_ready=true)")
                      else:
                          not_found.append(f"{model_name} (config_hash={config_hash})")
                          print(f"‚ö†Ô∏è  Not found: {model_name} with config_hash={config_hash} and production_ready tag")
                
                  if not_found:
                      print(f"\n‚ö†Ô∏è  Warning: {len(not_found)} model(s) not marked as production-ready:")
                      for model in not_found:
                          print(f"   - {model}")
                      print("\n‚ùå Cannot proceed with production deployment")
                      print("   All models must be tested and QA-approved first")
                      sys.exit(1)
                
                  if not models_info:
                      print("\n‚ùå No production-ready models found in Registry")
                      sys.exit(1)
                
                # Save models info
                  with open('registry_models.json', 'w') as f:
                      json.dump({'models': models_info}, f, indent=2)
                
                  print(f"\n‚úÖ Verified {len(models_info)} production-ready model(s) for deployment")
                  EOF
          
            - task: PublishPipelineArtifact@1
              inputs:
                targetPath: 'registry_models.json'
                artifact: 'registry_models'
  
    - stage: DeployToProduction
      displayName: '[PROD] Deploy to Production (Approval Required)'
      dependsOn: VerifyRegistryProd
      condition: ne(variables['Build.Reason'], 'PullRequest')  # Skip on PRs
      jobs:
        - deployment: DeployProd
          displayName: 'Senior ML Engineer + Platform Lead Approval'
          environment: 'production-deployment'
          pool:
            vmImage: 'ubuntu-latest'
          strategy:
            runOnce:
              deploy:
                steps:
                  - checkout: self
                
                  - task: DownloadPipelineArtifact@2
                    inputs:
                      artifact: 'registry_models'
                      path: $(Pipeline.Workspace)
                
                  - task: AzureCLI@2
                    displayName: 'Deploy Production Batch Endpoints from Registry'
                    inputs:
                      azureSubscription: '$(azureServiceConnection)'
                      scriptType: 'bash'
                      scriptLocation: 'inlineScript'
                      inlineScript: |
                        echo "‚úÖ Production deployment approved"
                        echo "üöÄ Deploying batch endpoints to Production..."
                      
                      # Deployment logic similar to test
                      
                        echo "‚úÖ All Production endpoints deployed"
  
    - stage: ProductionValidation
      displayName: '[PROD] Production Validation'
      dependsOn: DeployToProduction
      condition: ne(variables['Build.Reason'], 'PullRequest')  # Skip on PRs
      jobs:
        - job: SmokeTests
          displayName: 'Run Production Smoke Tests'
          pool:
            vmImage: 'ubuntu-latest'
          steps:
            - script: |
                echo "üß™ Running smoke tests on Production endpoints..."
                echo "‚úÖ Production validation passed"
              displayName: 'Smoke Tests'
